{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Spark and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('eurovisiontree').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('results_2017_2018.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- round: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- playing_order: integer (nullable = true)\n",
      " |-- rank_source: integer (nullable = true)\n",
      " |-- points_televoting: integer (nullable = true)\n",
      " |-- points_jury: integer (nullable = true)\n",
      " |-- tweets: integer (nullable = true)\n",
      " |-- positive: integer (nullable = true)\n",
      " |-- neutral: integer (nullable = true)\n",
      " |-- negative: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+\n",
      "|     round|country|playing_order|rank_source|points_televoting|points_jury|tweets|positive|neutral|negative|\n",
      "+----------+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+\n",
      "|2017_final|    POR|           11|          1|              376|        382|  3377|     833|    138|     786|\n",
      "|2017_final|    BUL|           25|          2|              337|        278|  1364|     488|    106|     313|\n",
      "|2017_final|    MDA|            7|          3|              264|        110|  2024|     875|     95|     438|\n",
      "|2017_final|    BEL|           23|          4|              255|        108|  1642|     409|    118|     494|\n",
      "|2017_final|    SWE|           24|          5|              126|        218|  1560|     448|    132|     519|\n",
      "+----------+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total points\n",
    "data_tmp1 = data.withColumn('points_total', data['points_televoting'] + data['points_jury'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rank columns\n",
    "\n",
    "# set windows for each rank\n",
    "total_rank_window = Window.partitionBy('round').orderBy(data_tmp1['points_total'].desc())\n",
    "jury_rank_window = Window.partitionBy('round').orderBy(data_tmp1['points_jury'].desc())\n",
    "televoting_rank_window = Window.partitionBy('round').orderBy(data_tmp1['points_televoting'].desc())\n",
    "\n",
    "# create rank columns\n",
    "data_tmp2 = data_tmp1\\\n",
    ".withColumn('rank_total', rank().over(total_rank_window))\\\n",
    ".withColumn('rank_jury', rank().over(jury_rank_window))\\\n",
    ".withColumn('rank_televoting', rank().over(televoting_rank_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute percentage of x_tweets per round\n",
    "round_window = Window.partitionBy('round').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "data_tmp3 = data_tmp2\\\n",
    ".withColumn('tweets_perc', data_tmp2['tweets']/sum('tweets').over(round_window))\\\n",
    ".withColumn('positive_perc', data_tmp2['positive']/sum('positive').over(round_window))\\\n",
    ".withColumn('negative_perc', data_tmp2['negative']/sum('negative').over(round_window))\\\n",
    ".withColumn('neutral_perc', data_tmp2['neutral']/sum('neutral').over(round_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of x_tweets\n",
    "data_tmp4 = data_tmp3\\\n",
    ".withColumn('tweets_log', log(data_tmp3['tweets']))\\\n",
    ".withColumn('positive_log', log(data_tmp3['positive']))\\\n",
    ".withColumn('negative_log', log(data_tmp3['negative']))\\\n",
    ".withColumn('neutral_log', log(data_tmp3['neutral']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize playing order\n",
    "data_tmp5 = data_tmp4\\\n",
    ".withColumn('playing_order_norm', data_tmp4['playing_order']/count('playing_order').over(round_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels 'isTopN'\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "isTop5 = udf(lambda r: 1 if r<=5 else 0)\n",
    "isTop10 = udf(lambda r: 1 if r<=10 else 0)\n",
    "    \n",
    "data_tmp6 = data_tmp5\\\n",
    ".withColumn('isTop5_jury', isTop5(data_tmp5['rank_jury']).cast(IntegerType()))\\\n",
    ".withColumn('isTop10_jury', isTop10(data_tmp5['rank_jury']).cast(IntegerType()))\\\n",
    ".withColumn('isTop5_televoting', isTop5(data_tmp5['rank_televoting']).cast(IntegerType()))\\\n",
    ".withColumn('isTop10_televoting', isTop10(data_tmp5['rank_televoting']).cast(IntegerType()))\\\n",
    ".withColumn('isTop5_total', isTop5(data_tmp5['rank_total']).cast(IntegerType()))\\\n",
    ".withColumn('isTop10_total', isTop10(data_tmp5['rank_total']).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_tmp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- round: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- playing_order: integer (nullable = true)\n",
      " |-- rank_source: integer (nullable = true)\n",
      " |-- points_televoting: integer (nullable = true)\n",
      " |-- points_jury: integer (nullable = true)\n",
      " |-- tweets: integer (nullable = true)\n",
      " |-- positive: integer (nullable = true)\n",
      " |-- neutral: integer (nullable = true)\n",
      " |-- negative: integer (nullable = true)\n",
      " |-- points_total: integer (nullable = true)\n",
      " |-- rank_total: integer (nullable = true)\n",
      " |-- rank_jury: integer (nullable = true)\n",
      " |-- rank_televoting: integer (nullable = true)\n",
      " |-- tweets_perc: double (nullable = true)\n",
      " |-- positive_perc: double (nullable = true)\n",
      " |-- negative_perc: double (nullable = true)\n",
      " |-- neutral_perc: double (nullable = true)\n",
      " |-- tweets_log: double (nullable = true)\n",
      " |-- positive_log: double (nullable = true)\n",
      " |-- negative_log: double (nullable = true)\n",
      " |-- neutral_log: double (nullable = true)\n",
      " |-- playing_order_norm: double (nullable = true)\n",
      " |-- isTop5_jury: integer (nullable = true)\n",
      " |-- isTop10_jury: integer (nullable = true)\n",
      " |-- isTop5_televoting: integer (nullable = true)\n",
      " |-- isTop10_televoting: integer (nullable = true)\n",
      " |-- isTop5_total: integer (nullable = true)\n",
      " |-- isTop10_total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>country</th>\n",
       "      <th>playing_order</th>\n",
       "      <th>rank_source</th>\n",
       "      <th>points_televoting</th>\n",
       "      <th>points_jury</th>\n",
       "      <th>tweets</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>...</th>\n",
       "      <th>positive_log</th>\n",
       "      <th>negative_log</th>\n",
       "      <th>neutral_log</th>\n",
       "      <th>playing_order_norm</th>\n",
       "      <th>isTop5_jury</th>\n",
       "      <th>isTop10_jury</th>\n",
       "      <th>isTop5_televoting</th>\n",
       "      <th>isTop10_televoting</th>\n",
       "      <th>isTop5_total</th>\n",
       "      <th>isTop10_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017_semi2</td>\n",
       "      <td>BUL</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>199</td>\n",
       "      <td>2014</td>\n",
       "      <td>741</td>\n",
       "      <td>73</td>\n",
       "      <td>427</td>\n",
       "      <td>...</td>\n",
       "      <td>6.608001</td>\n",
       "      <td>6.056784</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017_semi2</td>\n",
       "      <td>HUN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>66</td>\n",
       "      <td>1479</td>\n",
       "      <td>328</td>\n",
       "      <td>97</td>\n",
       "      <td>399</td>\n",
       "      <td>...</td>\n",
       "      <td>5.793014</td>\n",
       "      <td>5.988961</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017_semi2</td>\n",
       "      <td>ROU</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>26</td>\n",
       "      <td>379</td>\n",
       "      <td>95</td>\n",
       "      <td>24</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>4.584967</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017_semi2</td>\n",
       "      <td>ISR</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>75</td>\n",
       "      <td>2201</td>\n",
       "      <td>469</td>\n",
       "      <td>70</td>\n",
       "      <td>866</td>\n",
       "      <td>...</td>\n",
       "      <td>6.150603</td>\n",
       "      <td>6.763885</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017_semi2</td>\n",
       "      <td>CRO</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>104</td>\n",
       "      <td>37</td>\n",
       "      <td>2225</td>\n",
       "      <td>705</td>\n",
       "      <td>122</td>\n",
       "      <td>733</td>\n",
       "      <td>...</td>\n",
       "      <td>6.558198</td>\n",
       "      <td>6.597146</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        round country  playing_order  rank_source  points_televoting  \\\n",
       "0  2017_semi2     BUL             15            1                204   \n",
       "1  2017_semi2     HUN              7            2                165   \n",
       "2  2017_semi2     ROU              5            6                148   \n",
       "3  2017_semi2     ISR             18            3                132   \n",
       "4  2017_semi2     CRO             11            8                104   \n",
       "\n",
       "   points_jury  tweets  positive  neutral  negative      ...        \\\n",
       "0          199    2014       741       73       427      ...         \n",
       "1           66    1479       328       97       399      ...         \n",
       "2           26     379        95       24        98      ...         \n",
       "3           75    2201       469       70       866      ...         \n",
       "4           37    2225       705      122       733      ...         \n",
       "\n",
       "   positive_log  negative_log  neutral_log  playing_order_norm  isTop5_jury  \\\n",
       "0      6.608001      6.056784     4.290459            0.833333            1   \n",
       "1      5.793014      5.988961     4.574711            0.388889            0   \n",
       "2      4.553877      4.584967     3.178054            0.277778            0   \n",
       "3      6.150603      6.763885     4.248495            1.000000            0   \n",
       "4      6.558198      6.597146     4.804021            0.611111            0   \n",
       "\n",
       "   isTop10_jury  isTop5_televoting  isTop10_televoting  isTop5_total  \\\n",
       "0             1                  1                   1             1   \n",
       "1             1                  1                   1             1   \n",
       "2             0                  1                   1             0   \n",
       "3             1                  1                   1             1   \n",
       "4             0                  1                   1             0   \n",
       "\n",
       "   isTop10_total  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+------------+----------+---------+---------------+-----------+-------------+-------------+------------+----------+------------+------------+-----------+------------------+-----------+------------+-----------------+------------------+------------+-------------+\n",
      "|round|country|playing_order|rank_source|points_televoting|points_jury|tweets|positive|neutral|negative|points_total|rank_total|rank_jury|rank_televoting|tweets_perc|positive_perc|negative_perc|neutral_perc|tweets_log|positive_log|negative_log|neutral_log|playing_order_norm|isTop5_jury|isTop10_jury|isTop5_televoting|isTop10_televoting|isTop5_total|isTop10_total|\n",
      "+-----+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+------------+----------+---------+---------------+-----------+-------------+-------------+------------+----------+------------+------------+-----------+------------------+-----------+------------+-----------------+------------------+------------+-------------+\n",
      "|    0|      0|            0|          0|                0|          0|     0|       0|      0|       0|           0|         0|        0|              0|          0|            0|            0|           0|         0|           0|           0|          0|                 0|          0|           0|                0|                 0|           0|            0|\n",
      "+-----+-------+-------------+-----------+-----------------+-----------+------+--------+-------+--------+------------+----------+---------+---------------+-----------+-------------+-------------+------------+----------+------------+------------+-----------+------------------+-----------+------------+-----------------+------------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list number of NANs or NULLs in each column\n",
    "from pyspark.sql.functions import count, when, isnan, col\n",
    "data_features.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data_features.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to drop\n",
    "cleaned_data = data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set label to predict\n",
    "isTopN = 'isTop10_total'\n",
    "\n",
    "# set the features to analyze in the model\n",
    "#features = ['negative', 'neutral', 'positive', 'tweets']\n",
    "#features = ['negative_log', 'neutral_log', 'positive_log', 'tweets_log']\n",
    "features = ['negative_perc', 'neutral_perc', 'positive_perc', 'tweets_perc', 'playing_order_norm']\n",
    "#features = ['negative', 'neutral', 'positive', 'tweets', 'playing_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "prepared_data = assembler.transform(cleaned_data).select('features',isTopN)\n",
    "\n",
    "train_data,test_data = prepared_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake split with 2018_final as test \n",
    "train_data = assembler.transform(\n",
    "    cleaned_data\\\n",
    "    .filter(cleaned_data['round'] != '2018_final')\n",
    ")\\\n",
    ".select('round','country','features',isTopN)\n",
    "\n",
    "test_data = assembler.transform(\n",
    "    cleaned_data\\\n",
    "    .filter(cleaned_data['round'] == '2018_final')\n",
    ")\\\n",
    ".select('round','country','features',isTopN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 rows in train_data\n",
      "26 rows in test_data\n"
     ]
    }
   ],
   "source": [
    "print('%d rows in train_data' % train_data.count())\n",
    "print('%d rows in test_data' % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance\n",
      "[('negative_perc', 0.29978915158226155), ('neutral_perc', 0.12141528213931896), ('positive_perc', 0.3770670154930325), ('tweets_perc', 0.09809414412072545), ('playing_order_norm', 0.10363440666466152)]\n",
      "DTC AUC: 0.634375\n",
      "DTC ACC 0.653846\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create Classifier instances\n",
    "dtc = DecisionTreeClassifier(labelCol=isTopN,featuresCol='features')\n",
    "\n",
    "# train classifiers\n",
    "dtc_model = dtc.fit(train_data)\n",
    "\n",
    "# evaluate on test data\n",
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "\n",
    "# Feature importance\n",
    "print('Feature importance')\n",
    "print(list(zip(features, list(dtc_model.featureImportances))))\n",
    "\n",
    "# AUC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=isTopN)\n",
    "print('DTC AUC: %f' % binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "# Accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=isTopN, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print('DTC ACC %f' % acc_evaluator.evaluate(dtc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features raw\n",
    "  - DTC Default: AUC 0.63 - ACC 0.57\n",
    "\n",
    "- Features raw + playing order\n",
    "  - DTC Default: AUC 0.59 - ACC 0.65\n",
    "\n",
    "- Features percentages\n",
    "  - DTC Default: AUC 0.53 - ACC 0.50\n",
    "  - DTC maxDepth=10, maxBins=64: AUC 0.56 - ACC 0.50\n",
    " \n",
    "- Features percentages + playing order norm\n",
    "  - DTC Default: AUC 0.65 - ACC 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+-------------+-------------+--------------------+----------+\n",
      "|     round|country|            features|isTop10_total|rawPrediction|         probability|prediction|\n",
      "+----------+-------+--------------------+-------------+-------------+--------------------+----------+\n",
      "|2018_final|    AUS|[0.04969097651421...|            0|    [0.0,2.0]|           [0.0,1.0]|       1.0|\n",
      "|2018_final|    ITA|[0.01878862793572...|            1|    [2.0,5.0]|[0.28571428571428...|       1.0|\n",
      "|2018_final|    FIN|[0.02694684796044...|            0|    [2.0,5.0]|[0.28571428571428...|       1.0|\n",
      "|2018_final|    MDA|[0.03584672435105...|            1|    [2.0,5.0]|[0.28571428571428...|       1.0|\n",
      "|2018_final|    ISR|[0.11297898640296...|            1|    [0.0,2.0]|           [0.0,1.0]|       1.0|\n",
      "|2018_final|    CYP|[0.02793572311495...|            1|    [2.0,5.0]|[0.28571428571428...|       1.0|\n",
      "|2018_final|    IRL|[0.02126081582200...|            0|    [2.0,5.0]|[0.28571428571428...|       1.0|\n",
      "|2018_final|    AUT|[0.02175525339925...|            1|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    GER|[0.06328800988875...|            1|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    EST|[0.02917181705809...|            1|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    FRA|[0.02175525339925...|            0|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    CZE|[0.03559950556242...|            1|    [7.0,3.0]|           [0.7,0.3]|       0.0|\n",
      "|2018_final|    DEN|[0.05661310259579...|            1|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    ALB|[0.02768850432632...|            0|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    BUL|[0.03114956736711...|            0|   [11.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|2018_final|    NOR|[0.04573547589616...|            0|    [7.0,3.0]|           [0.7,0.3]|       0.0|\n",
      "|2018_final|    NED|[0.03362175525339...|            0|   [11.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|2018_final|    SWE|[0.03461063040791...|            1|   [11.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|2018_final|    LTU|[0.03831891223733...|            0|   [13.0,1.0]|[0.92857142857142...|       0.0|\n",
      "|2018_final|    UKR|[0.02818294190358...|            0|    [7.0,3.0]|           [0.7,0.3]|       0.0|\n",
      "+----------+-------+--------------------+-------------+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtc_predictions.orderBy(desc('prediction')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance\n",
      "[('negative_perc', 0.29978915158226155), ('neutral_perc', 0.12141528213931896), ('positive_perc', 0.3770670154930325), ('tweets_perc', 0.09809414412072545), ('playing_order_norm', 0.10363440666466152)]\n",
      "RFC AUC: 0.618750\n",
      "RFC accuracy 0.692308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create Classifier instances\n",
    "rfc = RandomForestClassifier(labelCol=isTopN,featuresCol='features', numTrees=100)\n",
    "\n",
    "# train classifiers\n",
    "rfc_model = rfc.fit(train_data)\n",
    "\n",
    "# Feature importance\n",
    "print('Feature importance')\n",
    "print(list(zip(features, list(dtc_model.featureImportances))))\n",
    "\n",
    "# evaluate on test data\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "\n",
    "# AUC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=isTopN)\n",
    "print('RFC AUC: %f' % binary_eval.evaluate(rfc_predictions))\n",
    "\n",
    "# Accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=isTopN, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print('RFC accuracy %f' % acc_evaluator.evaluate(rfc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features raw\n",
    "  - DTC Default: AUC 0.61 - ACC 0.60\n",
    "  - DTC 100 trees: AUC 0.60 - ACC 0.62\n",
    "\n",
    "- Features percentages\n",
    "  - DTC Default: AUC xxx - ACC xxx\n",
    "\n",
    "- Features percentages + playing order norm\n",
    "  - DTC 100 trees: AUC 0.82 - ACC 0.69 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|     round|country|            features|isTop10_total|       rawPrediction|         probability|prediction|\n",
      "+----------+-------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "|2018_final|    GBR|[0.08034610630407...|            0|[40.2684371184371...|[0.40268437118437...|       1.0|\n",
      "|2018_final|    ISR|[0.11297898640296...|            1|[18.8761904761904...|[0.18876190476190...|       1.0|\n",
      "|2018_final|    ITA|[0.01878862793572...|            1|[43.0704535843966...|[0.43070453584396...|       1.0|\n",
      "|2018_final|    MDA|[0.03584672435105...|            1|[43.7347261539323...|[0.43734726153932...|       1.0|\n",
      "|2018_final|    CYP|[0.02793572311495...|            1|[23.4833797387020...|[0.23483379738702...|       1.0|\n",
      "|2018_final|    IRL|[0.02126081582200...|            0|[39.4124545854239...|[0.39412454585423...|       1.0|\n",
      "|2018_final|    AUT|[0.02175525339925...|            1|[81.6857551361083...|[0.81685755136108...|       0.0|\n",
      "|2018_final|    GER|[0.06328800988875...|            1|[87.9119272545865...|[0.87911927254586...|       0.0|\n",
      "|2018_final|    DEN|[0.05661310259579...|            1|[78.8592796385959...|[0.78859279638596...|       0.0|\n",
      "|2018_final|    CZE|[0.03559950556242...|            1|[55.5238627515801...|[0.55523862751580...|       0.0|\n",
      "|2018_final|    SWE|[0.03461063040791...|            1|[82.2548552395880...|[0.82254855239588...|       0.0|\n",
      "|2018_final|    ALB|[0.02768850432632...|            0|[76.0697952421142...|[0.76069795242114...|       0.0|\n",
      "|2018_final|    LTU|[0.03831891223733...|            0|[89.9579520838529...|[0.89957952083852...|       0.0|\n",
      "|2018_final|    BUL|[0.03114956736711...|            0|[78.2836841310031...|[0.78283684131003...|       0.0|\n",
      "|2018_final|    UKR|[0.02818294190358...|            0|[67.8184422080713...|[0.67818442208071...|       0.0|\n",
      "|2018_final|    SRB|[0.01854140914709...|            0|[83.3040685853188...|[0.83304068585318...|       0.0|\n",
      "|2018_final|    EST|[0.02917181705809...|            1|[74.4822397930735...|[0.74482239793073...|       0.0|\n",
      "|2018_final|    FRA|[0.02175525339925...|            0|[84.6873092193650...|[0.84687309219365...|       0.0|\n",
      "|2018_final|    NOR|[0.04573547589616...|            0|[67.0346404688748...|[0.67034640468874...|       0.0|\n",
      "|2018_final|    NED|[0.03362175525339...|            0|[79.1479969692707...|[0.79147996969270...|       0.0|\n",
      "+----------+-------+--------------------+-------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_predictions.orderBy(desc('prediction')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
